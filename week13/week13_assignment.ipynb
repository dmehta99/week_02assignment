{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-fc1c39d0d6f8b82a",
          "locked": true,
          "schema_version": 1,
          "solution": false
        },
        "id": "2kIwebDWV4lT"
      },
      "source": [
        "# Week 13 Programming Assignment\n",
        "\n",
        "As you're working on identifying data for your final project, it's helpful to spend some time exploring your various data files.  For this week's assignment, I'd like you to take a few of the ideas from the the lectures, review assignment, and tonight's lecture to start exploring the data you plan to use for the final project.\n",
        "\n",
        "For the following activities, you can use just one of the data files that you've identified, or you can do the activities using different data files.  That's up to you.  Check in your data file along with this exercise. Please describe what you're doing in a Markdown cell or in the comments of your code.  When you've completed the assignment, submit it as normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBMaD45gV4lV"
      },
      "source": [
        "### 1. Basic Statistics\n",
        "\n",
        "Use Python to read in your data file and show many rows and columns your data has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FRlAJjGHV4lW",
        "outputId": "c84403a2-29a4-4ed3-c1df-deebddcae6f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted the zip file.\n",
            "Files extracted: ['.config', 'archive.zip', 'World CO2 Emission MetaData.csv', 'World CO2 Emission Data.csv', 'sample_data']\n",
            "The dataset has 8292 rows and 67 columns.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "  Country Name Country Code  \\\n",
            "0  Afghanistan          AFG   \n",
            "1  Afghanistan          AFG   \n",
            "2  Afghanistan          AFG   \n",
            "3  Afghanistan          AFG   \n",
            "4  Afghanistan          AFG   \n",
            "\n",
            "                                         Series Name           Series Code  \\\n",
            "0  Adjusted savings: carbon dioxide damage (% of ...     NY.ADJ.DCO2.GN.ZS   \n",
            "1  Adjusted savings: carbon dioxide damage (curre...        NY.ADJ.DCO2.CD   \n",
            "2  Agricultural methane emissions (thousand metri...  EN.ATM.METH.AG.KT.CE   \n",
            "3  Agricultural nitrous oxide emissions (thousand...  EN.ATM.NOXE.AG.KT.CE   \n",
            "4             CO2 emissions (kg per 2015 US$ of GDP)     EN.ATM.CO2E.KD.GD   \n",
            "\n",
            "  1960 [YR1960] 1961 [YR1961] 1962 [YR1962] 1963 [YR1963] 1964 [YR1964]  \\\n",
            "0            ..            ..            ..            ..            ..   \n",
            "1            ..            ..            ..            ..            ..   \n",
            "2            ..            ..            ..            ..            ..   \n",
            "3            ..            ..            ..            ..            ..   \n",
            "4            ..            ..            ..            ..            ..   \n",
            "\n",
            "  1965 [YR1965]  ...      2013 [YR2013]      2014 [YR2014]     2015 [YR2015]  \\\n",
            "0            ..  ...   1.36338448111679   1.28490840585691  1.44307895621429   \n",
            "1            ..  ...   275631280.588653   263338827.184962  278618004.122999   \n",
            "2            ..  ...           11284.75         11476.1975         10834.435   \n",
            "3            ..  ...          4440.1106          4744.2494         4702.3804   \n",
            "4            ..  ...  0.489964677117366  0.470845775464021  0.50292618070602   \n",
            "\n",
            "       2016 [YR2016]      2017 [YR2017]      2018 [YR2018]     2019 [YR2019]  \\\n",
            "0   1.44782682979766    1.4533391998403   1.59805470965195  1.55643923193288   \n",
            "1   264910281.973801   276138145.017514   291498572.368998   297253521.47533   \n",
            "2         10617.2325         10314.9575         10549.4125         10222.785   \n",
            "3          4680.2688          4892.1766          4289.9782         4258.4498   \n",
            "4  0.454516147902539  0.477468930733004  0.516563214552466  0.50918979595916   \n",
            "\n",
            "       2020 [YR2020] 2021 [YR2021] 2022 [YR2022]  \n",
            "0   1.40187835275711            ..            ..  \n",
            "1   284648920.796122            ..            ..  \n",
            "2           10679.11            ..            ..  \n",
            "3           4465.977            ..            ..  \n",
            "4  0.404094523787425            ..            ..  \n",
            "\n",
            "[5 rows x 67 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Correct file path, make sure to use quotes around the path\n",
        "file_path = '/content/archive.zip'  # Adjust this path to match the location of your file\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    # If the file is a .zip, you may need to extract it before reading the contents\n",
        "    if file_path.endswith('.zip'):\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            # Extract all contents to a folder\n",
        "            zip_ref.extractall('/content/')\n",
        "            print(\"Extracted the zip file.\")\n",
        "\n",
        "        # Now, check the contents of the zip file and choose the CSV or other relevant file\n",
        "        extracted_files = os.listdir('/content/')\n",
        "        print(\"Files extracted:\", extracted_files)\n",
        "\n",
        "        # If a CSV file is extracted, load it\n",
        "        csv_file = [file for file in extracted_files if file.endswith('.csv')][0]  # Pick the first CSV file\n",
        "        file_path = f'/content/{csv_file}'\n",
        "\n",
        "    # Read the data from the CSV file (use read_excel for Excel files)\n",
        "    try:\n",
        "        # Attempt to read with a common encoding (ISO-8859-1 or latin1)\n",
        "        data = pd.read_csv(file_path, encoding='ISO-8859-1')  # You can change encoding if necessary\n",
        "    except UnicodeDecodeError:\n",
        "        # In case there is still a decoding error, try another encoding\n",
        "        print(\"Unicode decoding error, trying a different encoding...\")\n",
        "        data = pd.read_csv(file_path, encoding='utf-16')  # Try utf-16 or another encoding\n",
        "\n",
        "    # Show the number of rows and columns\n",
        "    num_rows, num_columns = data.shape\n",
        "    print(f'The dataset has {num_rows} rows and {num_columns} columns.')\n",
        "\n",
        "    # Display the first few rows of the dataset\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())  # Shows the first 5 rows, can change to .tail() for last 5 rows\n",
        "\n",
        "else:\n",
        "    print(f\"Error: The file '{file_path}' does not exist. Please check the file path.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DRmbMOmV4lX"
      },
      "source": [
        "### 2. Data Diversity\n",
        "\n",
        "Use Python to identify how many unique values each column in your data has.  Use Python to identify which column has the greatest number of distinct values (aka the most diversity)?  If your data has a unique identifier (e.g. a person or visit or record ID) then find the column with the next most number of unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tmqa8RuNV4lX",
        "outputId": "fcbedfeb-6e67-4b81-aa6f-e8fb10ab1754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted the zip file.\n",
            "Files extracted: ['.config', 'archive.zip', 'World CO2 Emission MetaData.csv', 'World CO2 Emission Data.csv', 'sample_data']\n",
            "The dataset has 8292 rows and 67 columns.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "  Country Name Country Code  \\\n",
            "0  Afghanistan          AFG   \n",
            "1  Afghanistan          AFG   \n",
            "2  Afghanistan          AFG   \n",
            "3  Afghanistan          AFG   \n",
            "4  Afghanistan          AFG   \n",
            "\n",
            "                                         Series Name           Series Code  \\\n",
            "0  Adjusted savings: carbon dioxide damage (% of ...     NY.ADJ.DCO2.GN.ZS   \n",
            "1  Adjusted savings: carbon dioxide damage (curre...        NY.ADJ.DCO2.CD   \n",
            "2  Agricultural methane emissions (thousand metri...  EN.ATM.METH.AG.KT.CE   \n",
            "3  Agricultural nitrous oxide emissions (thousand...  EN.ATM.NOXE.AG.KT.CE   \n",
            "4             CO2 emissions (kg per 2015 US$ of GDP)     EN.ATM.CO2E.KD.GD   \n",
            "\n",
            "  1960 [YR1960] 1961 [YR1961] 1962 [YR1962] 1963 [YR1963] 1964 [YR1964]  \\\n",
            "0            ..            ..            ..            ..            ..   \n",
            "1            ..            ..            ..            ..            ..   \n",
            "2            ..            ..            ..            ..            ..   \n",
            "3            ..            ..            ..            ..            ..   \n",
            "4            ..            ..            ..            ..            ..   \n",
            "\n",
            "  1965 [YR1965]  ...      2013 [YR2013]      2014 [YR2014]     2015 [YR2015]  \\\n",
            "0            ..  ...   1.36338448111679   1.28490840585691  1.44307895621429   \n",
            "1            ..  ...   275631280.588653   263338827.184962  278618004.122999   \n",
            "2            ..  ...           11284.75         11476.1975         10834.435   \n",
            "3            ..  ...          4440.1106          4744.2494         4702.3804   \n",
            "4            ..  ...  0.489964677117366  0.470845775464021  0.50292618070602   \n",
            "\n",
            "       2016 [YR2016]      2017 [YR2017]      2018 [YR2018]     2019 [YR2019]  \\\n",
            "0   1.44782682979766    1.4533391998403   1.59805470965195  1.55643923193288   \n",
            "1   264910281.973801   276138145.017514   291498572.368998   297253521.47533   \n",
            "2         10617.2325         10314.9575         10549.4125         10222.785   \n",
            "3          4680.2688          4892.1766          4289.9782         4258.4498   \n",
            "4  0.454516147902539  0.477468930733004  0.516563214552466  0.50918979595916   \n",
            "\n",
            "       2020 [YR2020] 2021 [YR2021] 2022 [YR2022]  \n",
            "0   1.40187835275711            ..            ..  \n",
            "1   284648920.796122            ..            ..  \n",
            "2           10679.11            ..            ..  \n",
            "3           4465.977            ..            ..  \n",
            "4  0.404094523787425            ..            ..  \n",
            "\n",
            "[5 rows x 67 columns]\n",
            "\n",
            "Number of unique values in each column:\n",
            "Country Name      302\n",
            "Country Code      271\n",
            "Series Name        34\n",
            "Series Code        55\n",
            "1960 [YR1960]     758\n",
            "                 ... \n",
            "2018 [YR2018]    3145\n",
            "2019 [YR2019]    3154\n",
            "2020 [YR2020]    3148\n",
            "2021 [YR2021]       4\n",
            "2022 [YR2022]       1\n",
            "Length: 67, dtype: int64\n",
            "\n",
            "The column with the most unique values is '2005 [YR2005]' with 5735 unique values.\n",
            "\n",
            "The second most diverse column is '2008 [YR2008]' with 5726 unique values.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Correct file path, make sure to use quotes around the path\n",
        "file_path = '/content/archive.zip'  # Adjust this path to match the location of your file\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    # If the file is a .zip, you may need to extract it before reading the contents\n",
        "    if file_path.endswith('.zip'):\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            # Extract all contents to a folder\n",
        "            zip_ref.extractall('/content/')\n",
        "            print(\"Extracted the zip file.\")\n",
        "\n",
        "        # Now, check the contents of the zip file and choose the CSV or other relevant file\n",
        "        extracted_files = os.listdir('/content/')\n",
        "        print(\"Files extracted:\", extracted_files)\n",
        "\n",
        "        # If a CSV file is extracted, load it\n",
        "        csv_file = [file for file in extracted_files if file.endswith('.csv')][0]  # Pick the first CSV file\n",
        "        file_path = f'/content/{csv_file}'\n",
        "\n",
        "    # Read the data from the CSV file (use read_excel for Excel files)\n",
        "    try:\n",
        "        # Attempt to read with a common encoding (ISO-8859-1 or latin1)\n",
        "        data = pd.read_csv(file_path, encoding='ISO-8859-1')  # You can change encoding if necessary\n",
        "    except UnicodeDecodeError:\n",
        "        # In case there is still a decoding error, try another encoding\n",
        "        print(\"Unicode decoding error, trying a different encoding...\")\n",
        "        data = pd.read_csv(file_path, encoding='utf-16')  # Try utf-16 or another encoding\n",
        "\n",
        "    # Show the number of rows and columns\n",
        "    num_rows, num_columns = data.shape\n",
        "    print(f'The dataset has {num_rows} rows and {num_columns} columns.')\n",
        "\n",
        "    # Display the first few rows of the dataset\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())  # Shows the first 5 rows, can change to .tail() for last 5 rows\n",
        "\n",
        "    # Identify the number of unique values for each column\n",
        "    unique_counts = data.nunique()\n",
        "    print(\"\\nNumber of unique values in each column:\")\n",
        "    print(unique_counts)\n",
        "\n",
        "    # Find the column with the greatest number of unique values (most diverse)\n",
        "    most_diverse_column = unique_counts.idxmax()  # Get the column name with the max unique values\n",
        "    most_diverse_count = unique_counts.max()  # Get the number of unique values in that column\n",
        "\n",
        "    print(f\"\\nThe column with the most unique values is '{most_diverse_column}' with {most_diverse_count} unique values.\")\n",
        "\n",
        "    # If your data has a unique identifier, identify the next most diverse column\n",
        "    # Assuming the identifier column might be something like \"ID\", \"RecordID\", etc.\n",
        "    # We'll sort the columns by unique value count to find the second most diverse one.\n",
        "    sorted_unique_counts = unique_counts.sort_values(ascending=False)\n",
        "\n",
        "    # Get the second most diverse column, skipping the first one (the identifier)\n",
        "    second_most_diverse_column = sorted_unique_counts.index[1]\n",
        "    second_most_diverse_count = sorted_unique_counts.iloc[1]\n",
        "\n",
        "    print(f\"\\nThe second most diverse column is '{second_most_diverse_column}' with {second_most_diverse_count} unique values.\")\n",
        "else:\n",
        "    print(f\"Error: The file '{file_path}' does not exist. Please check the file path.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O4-vR72V4lY"
      },
      "source": [
        "### 3. Data Redundancy\n",
        "\n",
        "Use Python to identify any columns in your data where the value for every row is the same.  For example, if you had a gender column and all the rows said 'M', then you would want to write the code that helped you determine that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MaKihaPEV4lY",
        "outputId": "f47f5b7f-2ae3-4b5c-9f9f-3e1eff7f6a42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted the zip file.\n",
            "Files extracted: ['.config', 'archive.zip', 'World CO2 Emission MetaData.csv', 'World CO2 Emission Data.csv', 'sample_data']\n",
            "The dataset has 8292 rows and 67 columns.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "  Country Name Country Code  \\\n",
            "0  Afghanistan          AFG   \n",
            "1  Afghanistan          AFG   \n",
            "2  Afghanistan          AFG   \n",
            "3  Afghanistan          AFG   \n",
            "4  Afghanistan          AFG   \n",
            "\n",
            "                                         Series Name           Series Code  \\\n",
            "0  Adjusted savings: carbon dioxide damage (% of ...     NY.ADJ.DCO2.GN.ZS   \n",
            "1  Adjusted savings: carbon dioxide damage (curre...        NY.ADJ.DCO2.CD   \n",
            "2  Agricultural methane emissions (thousand metri...  EN.ATM.METH.AG.KT.CE   \n",
            "3  Agricultural nitrous oxide emissions (thousand...  EN.ATM.NOXE.AG.KT.CE   \n",
            "4             CO2 emissions (kg per 2015 US$ of GDP)     EN.ATM.CO2E.KD.GD   \n",
            "\n",
            "  1960 [YR1960] 1961 [YR1961] 1962 [YR1962] 1963 [YR1963] 1964 [YR1964]  \\\n",
            "0            ..            ..            ..            ..            ..   \n",
            "1            ..            ..            ..            ..            ..   \n",
            "2            ..            ..            ..            ..            ..   \n",
            "3            ..            ..            ..            ..            ..   \n",
            "4            ..            ..            ..            ..            ..   \n",
            "\n",
            "  1965 [YR1965]  ...      2013 [YR2013]      2014 [YR2014]     2015 [YR2015]  \\\n",
            "0            ..  ...   1.36338448111679   1.28490840585691  1.44307895621429   \n",
            "1            ..  ...   275631280.588653   263338827.184962  278618004.122999   \n",
            "2            ..  ...           11284.75         11476.1975         10834.435   \n",
            "3            ..  ...          4440.1106          4744.2494         4702.3804   \n",
            "4            ..  ...  0.489964677117366  0.470845775464021  0.50292618070602   \n",
            "\n",
            "       2016 [YR2016]      2017 [YR2017]      2018 [YR2018]     2019 [YR2019]  \\\n",
            "0   1.44782682979766    1.4533391998403   1.59805470965195  1.55643923193288   \n",
            "1   264910281.973801   276138145.017514   291498572.368998   297253521.47533   \n",
            "2         10617.2325         10314.9575         10549.4125         10222.785   \n",
            "3          4680.2688          4892.1766          4289.9782         4258.4498   \n",
            "4  0.454516147902539  0.477468930733004  0.516563214552466  0.50918979595916   \n",
            "\n",
            "       2020 [YR2020] 2021 [YR2021] 2022 [YR2022]  \n",
            "0   1.40187835275711            ..            ..  \n",
            "1   284648920.796122            ..            ..  \n",
            "2           10679.11            ..            ..  \n",
            "3           4465.977            ..            ..  \n",
            "4  0.404094523787425            ..            ..  \n",
            "\n",
            "[5 rows x 67 columns]\n",
            "\n",
            "The following columns have redundant values (same value for every row):\n",
            "Index(['2022 [YR2022]'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Correct file path, make sure to use quotes around the path\n",
        "file_path = '/content/archive.zip'  # Adjust this path to match the location of your file\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    # If the file is a .zip, you may need to extract it before reading the contents\n",
        "    if file_path.endswith('.zip'):\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            # Extract all contents to a folder\n",
        "            zip_ref.extractall('/content/')\n",
        "            print(\"Extracted the zip file.\")\n",
        "\n",
        "        # Now, check the contents of the zip file and choose the CSV or other relevant file\n",
        "        extracted_files = os.listdir('/content/')\n",
        "        print(\"Files extracted:\", extracted_files)\n",
        "\n",
        "        # If a CSV file is extracted, load it\n",
        "        csv_file = [file for file in extracted_files if file.endswith('.csv')][0]  # Pick the first CSV file\n",
        "        file_path = f'/content/{csv_file}'\n",
        "\n",
        "    # Read the data from the CSV file (use read_excel for Excel files)\n",
        "    try:\n",
        "        # Attempt to read with a common encoding (ISO-8859-1 or latin1)\n",
        "        data = pd.read_csv(file_path, encoding='ISO-8859-1')  # You can change encoding if necessary\n",
        "    except UnicodeDecodeError:\n",
        "        # In case there is still a decoding error, try another encoding\n",
        "        print(\"Unicode decoding error, trying a different encoding...\")\n",
        "        data = pd.read_csv(file_path, encoding='utf-16')  # Try utf-16 or another encoding\n",
        "\n",
        "    # Show the number of rows and columns\n",
        "    num_rows, num_columns = data.shape\n",
        "    print(f'The dataset has {num_rows} rows and {num_columns} columns.')\n",
        "\n",
        "    # Display the first few rows of the dataset\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())  # Shows the first 5 rows, can change to .tail() for last 5 rows\n",
        "\n",
        "    # Identify columns where every row has the same value (i.e., redundant columns)\n",
        "    redundant_columns = data.columns[data.nunique() == 1]  # Columns with only one unique value\n",
        "\n",
        "    if len(redundant_columns) > 0:\n",
        "        print(\"\\nThe following columns have redundant values (same value for every row):\")\n",
        "        print(redundant_columns)\n",
        "    else:\n",
        "        print(\"\\nNo redundant columns found (every column has variation).\")\n",
        "else:\n",
        "    print(f\"Error: The file '{file_path}' does not exist. Please check the file path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA3hJwtmV4lZ"
      },
      "source": [
        "### 4. Range of Values\n",
        "\n",
        "Your data almost certainly has some numeric columns that can be summed or averaged.  Create a histogram that shows the distribution of values for this column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HRz63GMhV4lZ",
        "outputId": "31d07755-a1f4-4aa2-9c95-8164c81916d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted the zip file.\n",
            "Files extracted: ['.config', 'archive.zip', 'World CO2 Emission MetaData.csv', 'World CO2 Emission Data.csv', 'sample_data']\n",
            "The dataset has 8292 rows and 67 columns.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "  Country Name Country Code  \\\n",
            "0  Afghanistan          AFG   \n",
            "1  Afghanistan          AFG   \n",
            "2  Afghanistan          AFG   \n",
            "3  Afghanistan          AFG   \n",
            "4  Afghanistan          AFG   \n",
            "\n",
            "                                         Series Name           Series Code  \\\n",
            "0  Adjusted savings: carbon dioxide damage (% of ...     NY.ADJ.DCO2.GN.ZS   \n",
            "1  Adjusted savings: carbon dioxide damage (curre...        NY.ADJ.DCO2.CD   \n",
            "2  Agricultural methane emissions (thousand metri...  EN.ATM.METH.AG.KT.CE   \n",
            "3  Agricultural nitrous oxide emissions (thousand...  EN.ATM.NOXE.AG.KT.CE   \n",
            "4             CO2 emissions (kg per 2015 US$ of GDP)     EN.ATM.CO2E.KD.GD   \n",
            "\n",
            "  1960 [YR1960] 1961 [YR1961] 1962 [YR1962] 1963 [YR1963] 1964 [YR1964]  \\\n",
            "0            ..            ..            ..            ..            ..   \n",
            "1            ..            ..            ..            ..            ..   \n",
            "2            ..            ..            ..            ..            ..   \n",
            "3            ..            ..            ..            ..            ..   \n",
            "4            ..            ..            ..            ..            ..   \n",
            "\n",
            "  1965 [YR1965]  ...      2013 [YR2013]      2014 [YR2014]     2015 [YR2015]  \\\n",
            "0            ..  ...   1.36338448111679   1.28490840585691  1.44307895621429   \n",
            "1            ..  ...   275631280.588653   263338827.184962  278618004.122999   \n",
            "2            ..  ...           11284.75         11476.1975         10834.435   \n",
            "3            ..  ...          4440.1106          4744.2494         4702.3804   \n",
            "4            ..  ...  0.489964677117366  0.470845775464021  0.50292618070602   \n",
            "\n",
            "       2016 [YR2016]      2017 [YR2017]      2018 [YR2018]     2019 [YR2019]  \\\n",
            "0   1.44782682979766    1.4533391998403   1.59805470965195  1.55643923193288   \n",
            "1   264910281.973801   276138145.017514   291498572.368998   297253521.47533   \n",
            "2         10617.2325         10314.9575         10549.4125         10222.785   \n",
            "3          4680.2688          4892.1766          4289.9782         4258.4498   \n",
            "4  0.454516147902539  0.477468930733004  0.516563214552466  0.50918979595916   \n",
            "\n",
            "       2020 [YR2020] 2021 [YR2021] 2022 [YR2022]  \n",
            "0   1.40187835275711            ..            ..  \n",
            "1   284648920.796122            ..            ..  \n",
            "2           10679.11            ..            ..  \n",
            "3           4465.977            ..            ..  \n",
            "4  0.404094523787425            ..            ..  \n",
            "\n",
            "[5 rows x 67 columns]\n",
            "\n",
            "Numeric columns in the dataset:\n",
            "Index([], dtype='object')\n",
            "\n",
            "The 'CO2_Emission' column is not found in the dataset.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Correct file path, make sure to use quotes around the path\n",
        "file_path = '/content/archive.zip'  # Adjust this path to match the location of your file\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(file_path):\n",
        "    # If the file is a .zip, you may need to extract it before reading the contents\n",
        "    if file_path.endswith('.zip'):\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            # Extract all contents to a folder\n",
        "            zip_ref.extractall('/content/')\n",
        "            print(\"Extracted the zip file.\")\n",
        "\n",
        "        # Now, check the contents of the zip file and choose the CSV or other relevant file\n",
        "        extracted_files = os.listdir('/content/')\n",
        "        print(\"Files extracted:\", extracted_files)\n",
        "\n",
        "        # If a CSV file is extracted, load it\n",
        "        csv_file = [file for file in extracted_files if file.endswith('.csv')][0]  # Pick the first CSV file\n",
        "        file_path = f'/content/{csv_file}'\n",
        "\n",
        "    # Read the data from the CSV file (use read_excel for Excel files)\n",
        "    try:\n",
        "        # Attempt to read with a common encoding (ISO-8859-1 or latin1)\n",
        "        data = pd.read_csv(file_path, encoding='ISO-8859-1')  # You can change encoding if necessary\n",
        "    except UnicodeDecodeError:\n",
        "        # In case there is still a decoding error, try another encoding\n",
        "        print(\"Unicode decoding error, trying a different encoding...\")\n",
        "        data = pd.read_csv(file_path, encoding='utf-16')  # Try utf-16 or another encoding\n",
        "\n",
        "    # Show the number of rows and columns\n",
        "    num_rows, num_columns = data.shape\n",
        "    print(f'The dataset has {num_rows} rows and {num_columns} columns.')\n",
        "\n",
        "    # Display the first few rows of the dataset\n",
        "    print(\"\\nFirst 5 rows of the dataset:\")\n",
        "    print(data.head())  # Shows the first 5 rows, can change to .tail() for last 5 rows\n",
        "\n",
        "    # Select only numeric columns\n",
        "    numeric_columns = data.select_dtypes(include=['number']).columns\n",
        "    print(\"\\nNumeric columns in the dataset:\")\n",
        "    print(numeric_columns)\n",
        "\n",
        "    # Choose a numeric column (for example, 'CO2_Emission') to plot a histogram\n",
        "    if 'CO2_Emission' in numeric_columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.histplot(data['CO2_Emission'], kde=True, bins=30)  # KDE for smooth density curve\n",
        "        plt.title('Distribution of CO2 Emissions')\n",
        "        plt.xlabel('CO2 Emission')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\nThe 'CO2_Emission' column is not found in the dataset.\")\n",
        "else:\n",
        "    print(f\"Error: The file '{file_path}' does not exist. Please check the file path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMqO7suBV4la"
      },
      "source": [
        "### 5. Discussion\n",
        "\n",
        "Do any additional exploration of this data that you think may be interesting and include your code here.  Then go onto Slack and make a post about what you've observed.  Share your observation.  Share your code, if you like.  Share a screenshot of the data or a chart based on the data.\n",
        "\n",
        "Then comment on at least two other people's observations that they share on Slack.  (Use the reply as thread option in Slack to keep the conversation organized.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zsA1mITCV4la",
        "outputId": "77162cd6-5a06-4c21-a582-4232d39617c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No numeric columns found in the dataset.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure the columns are numeric and drop any rows with missing values\n",
        "numeric_columns = data.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Check if there are numeric columns\n",
        "if len(numeric_columns) > 0:\n",
        "    # Drop rows with NaN values in any numeric column\n",
        "    data_clean = data[numeric_columns].dropna()\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    corr_matrix = data_clean.corr()\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "    plt.title('Correlation Heatmap of Numeric Columns')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No numeric columns found in the dataset.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Remove leading/trailing spaces from column names (if any)\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Check the column names\n",
        "print(data.columns)  # Print the column names to verify the correct name for CO2_Emission\n",
        "\n",
        "# Ensure the columns are numeric and drop any rows with missing values\n",
        "numeric_columns = data.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Check if there are numeric columns\n",
        "if len(numeric_columns) > 0:\n",
        "    # Drop rows with NaN values in any numeric column\n",
        "    data_clean = data[numeric_columns].dropna()\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    corr_matrix = data_clean.corr()\n",
        "\n",
        "    # Plot the heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "    plt.title('Correlation Heatmap of Numeric Columns')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No numeric columns found in the dataset.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "698UBrxiytOY",
        "outputId": "747b7414-feb5-46f8-c35a-39eb28555d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Country Name', 'Country Code', 'Series Name', 'Series Code',\n",
            "       '1960 [YR1960]', '1961 [YR1961]', '1962 [YR1962]', '1963 [YR1963]',\n",
            "       '1964 [YR1964]', '1965 [YR1965]', '1966 [YR1966]', '1967 [YR1967]',\n",
            "       '1968 [YR1968]', '1969 [YR1969]', '1970 [YR1970]', '1971 [YR1971]',\n",
            "       '1972 [YR1972]', '1973 [YR1973]', '1974 [YR1974]', '1975 [YR1975]',\n",
            "       '1976 [YR1976]', '1977 [YR1977]', '1978 [YR1978]', '1979 [YR1979]',\n",
            "       '1980 [YR1980]', '1981 [YR1981]', '1982 [YR1982]', '1983 [YR1983]',\n",
            "       '1984 [YR1984]', '1985 [YR1985]', '1986 [YR1986]', '1987 [YR1987]',\n",
            "       '1988 [YR1988]', '1989 [YR1989]', '1990 [YR1990]', '1991 [YR1991]',\n",
            "       '1992 [YR1992]', '1993 [YR1993]', '1994 [YR1994]', '1995 [YR1995]',\n",
            "       '1996 [YR1996]', '1997 [YR1997]', '1998 [YR1998]', '1999 [YR1999]',\n",
            "       '2000 [YR2000]', '2001 [YR2001]', '2002 [YR2002]', '2003 [YR2003]',\n",
            "       '2004 [YR2004]', '2005 [YR2005]', '2006 [YR2006]', '2007 [YR2007]',\n",
            "       '2008 [YR2008]', '2009 [YR2009]', '2010 [YR2010]', '2011 [YR2011]',\n",
            "       '2012 [YR2012]', '2013 [YR2013]', '2014 [YR2014]', '2015 [YR2015]',\n",
            "       '2016 [YR2016]', '2017 [YR2017]', '2018 [YR2018]', '2019 [YR2019]',\n",
            "       '2020 [YR2020]', '2021 [YR2021]', '2022 [YR2022]'],\n",
            "      dtype='object')\n",
            "No numeric columns found in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Remove leading/trailing spaces from column names (if any)\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Check the column names\n",
        "print(data.columns)  # Print the column names to verify the correct name for CO2_Emission\n",
        "\n",
        "# Ensure the columns are numeric and drop any rows with missing values\n",
        "numeric_columns = data.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Step 1: Correlation Heatmap\n",
        "if len(numeric_columns) > 0:\n",
        "    # Drop rows with NaN values in any numeric column\n",
        "    data_clean = data[numeric_columns].dropna()\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    corr_matrix = data_clean.corr()\n",
        "\n",
        "    # Plot the correlation heatmap\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "    plt.title('Correlation Heatmap of Numeric Columns')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No numeric columns found in the dataset.\")\n",
        "\n",
        "# Step 2: Histogram for CO2 Emission Data (if 'CO2_Emission' column exists)\n",
        "if 'CO2_Emission' in data.columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(data['CO2_Emission'], bins=20, kde=True, color='blue')\n",
        "    plt.title('Distribution of CO2 Emissions')\n",
        "    plt.xlabel('CO2 Emission')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Column 'CO2_Emission' not found in the dataset.\")\n"
      ],
      "metadata": {
        "id": "skuqRDgPzOFk",
        "outputId": "aa3be316-9ce6-452b-e9d8-a69c2cd0ff5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Country Name', 'Country Code', 'Series Name', 'Series Code',\n",
            "       '1960 [YR1960]', '1961 [YR1961]', '1962 [YR1962]', '1963 [YR1963]',\n",
            "       '1964 [YR1964]', '1965 [YR1965]', '1966 [YR1966]', '1967 [YR1967]',\n",
            "       '1968 [YR1968]', '1969 [YR1969]', '1970 [YR1970]', '1971 [YR1971]',\n",
            "       '1972 [YR1972]', '1973 [YR1973]', '1974 [YR1974]', '1975 [YR1975]',\n",
            "       '1976 [YR1976]', '1977 [YR1977]', '1978 [YR1978]', '1979 [YR1979]',\n",
            "       '1980 [YR1980]', '1981 [YR1981]', '1982 [YR1982]', '1983 [YR1983]',\n",
            "       '1984 [YR1984]', '1985 [YR1985]', '1986 [YR1986]', '1987 [YR1987]',\n",
            "       '1988 [YR1988]', '1989 [YR1989]', '1990 [YR1990]', '1991 [YR1991]',\n",
            "       '1992 [YR1992]', '1993 [YR1993]', '1994 [YR1994]', '1995 [YR1995]',\n",
            "       '1996 [YR1996]', '1997 [YR1997]', '1998 [YR1998]', '1999 [YR1999]',\n",
            "       '2000 [YR2000]', '2001 [YR2001]', '2002 [YR2002]', '2003 [YR2003]',\n",
            "       '2004 [YR2004]', '2005 [YR2005]', '2006 [YR2006]', '2007 [YR2007]',\n",
            "       '2008 [YR2008]', '2009 [YR2009]', '2010 [YR2010]', '2011 [YR2011]',\n",
            "       '2012 [YR2012]', '2013 [YR2013]', '2014 [YR2014]', '2015 [YR2015]',\n",
            "       '2016 [YR2016]', '2017 [YR2017]', '2018 [YR2018]', '2019 [YR2019]',\n",
            "       '2020 [YR2020]', '2021 [YR2021]', '2022 [YR2022]'],\n",
            "      dtype='object')\n",
            "No numeric columns found in the dataset.\n",
            "Column 'CO2_Emission' not found in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxZeL3TXV4la"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Submitting Your Work\n",
        "\n",
        "Submit your work as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwklcjvLV4lb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}